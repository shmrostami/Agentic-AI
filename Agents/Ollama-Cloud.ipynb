{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590c27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Short answer:** The sky looks blue because molecules in Earth’s atmosphere scatter short‑wavelength (blue) light from the Sun much more efficiently than they scatter long‑wavelength (red) light. This effect is called *Rayleigh scattering*.\n",
       "\n",
       "---\n",
       "\n",
       "### How it works, step by step\n",
       "\n",
       "| Step | What happens |\n",
       "|------|--------------|\n",
       "| **1. Sunlight reaches Earth** | Sunlight is a mixture of all visible colors (red, orange, yellow, green, blue, indigo, violet). Together they appear white. |\n",
       "| **2. Light meets the atmosphere** | The atmosphere is filled with tiny particles: nitrogen (~78 %), oxygen (~21 %), and trace gases, plus occasional dust, water droplets, etc. |\n",
       "| **3. Rayleigh scattering** | When light hits particles **much smaller than its wavelength** (the case for N₂ and O₂ molecules), the scattering intensity follows an inverse‑fourth‑power law: <br> **I ∝ 1/λ⁴** <br>Thus, blue light (λ ≈ 450 nm) is scattered ~ (650/450)⁴ ≈ **10 times more** than red light (λ ≈ 650 nm). |\n",
       "| **4. Scattered light reaches your eyes** | The scattered blue photons are sent in all directions, including toward you wherever you look. Because the blue component has been added to the light coming from every patch of sky, the whole dome appears blue. |\n",
       "| **5. Why not violet?** | Violet is scattered even more than blue, but: <br>• Human eyes are less sensitive to violet. <br>• Some violet is absorbed by the upper atmosphere’s ozone. <br>Result: the sky looks blue, not violet. |\n",
       "\n",
       "---\n",
       "\n",
       "### Why the sky changes color at sunrise and sunset\n",
       "\n",
       "When the Sun is low on the horizon, its light must travel through **much more atmosphere** (up to 40 × the vertical path). The short‑wavelength blue light gets scattered out of the direct line of sight long before the light reaches you, leaving a higher proportion of reds and oranges to reach your eyes directly. The remaining scattered light that does reach you from the sky is then tinted orange/red, giving the spectacular sunrise/sunset colors.\n",
       "\n",
       "---\n",
       "\n",
       "### Quick “back‑of‑the‑envelope” estimate\n",
       "\n",
       "The Rayleigh scattering cross‑section σ for a molecule scales as  \n",
       "\n",
       "\\[\n",
       "\\sigma \\propto \\frac{1}{\\lambda^{4}}\n",
       "\\]\n",
       "\n",
       "If you compare blue (λ ≈ 450 nm) to red (λ ≈ 650 nm):\n",
       "\n",
       "\\[\n",
       "\\frac{\\sigma_{\\text{blue}}}{\\sigma_{\\text{red}}}= \\left(\\frac{650}{450}\\right)^{4}\\approx 10\n",
       "\\]\n",
       "\n",
       "So a photon of blue light is roughly **10 times more likely** to be scattered than a photon of red light.\n",
       "\n",
       "---\n",
       "\n",
       "### TL;DR\n",
       "\n",
       "*Sunlight is white, but Earth’s tiny atmospheric molecules preferentially scatter short‑wavelength (blue) light because of Rayleigh scattering (intensity ∝ 1/λ⁴). The scattered blue light reaches us from every direction, making the sky appear blue.* \n",
       "\n",
       "---\n",
       "\n",
       "**Want to dive deeper?**  \n",
       "- The exact scattering formula (including the refractive index of air).  \n",
       "- How aerosols and dust lead to a whiter or hazier sky.  \n",
       "- Why other planets (e.g., Mars) have different sky colors.  \n",
       "\n",
       "Just let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "from ollama import Client\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "client = Client(host='https://ollama.com',\n",
    "            headers={'Authorization': f'Bearer {os.getenv(\"OLLAMA_API_KEY\")}'}\n",
    ")\n",
    "\n",
    "messages = [\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "]\n",
    "\n",
    "response = client.chat('gpt-oss:120b-cloud', messages=messages, stream=False)\n",
    "display(Markdown(response['message']['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7aad3dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cogito-2.1:671b',\n",
       " 'glm-4.6',\n",
       " 'glm-4.7',\n",
       " 'kimi-k2:1t',\n",
       " 'kimi-k2-thinking',\n",
       " 'qwen3-coder:480b',\n",
       " 'qwen3-next:80b',\n",
       " 'deepseek-v3.2',\n",
       " 'deepseek-v3.1:671b',\n",
       " 'gpt-oss:120b',\n",
       " 'nemotron-3-nano:30b',\n",
       " 'gpt-oss:20b',\n",
       " 'qwen3-vl:235b-instruct',\n",
       " 'qwen3-vl:235b',\n",
       " 'minimax-m2',\n",
       " 'minimax-m2.1',\n",
       " 'ministral-3:3b',\n",
       " 'ministral-3:8b',\n",
       " 'ministral-3:14b',\n",
       " 'mistral-large-3:675b',\n",
       " 'devstral-2:123b',\n",
       " 'devstral-small-2:24b',\n",
       " 'gemini-3-pro-preview',\n",
       " 'gemini-3-flash-preview',\n",
       " 'gemma3:4b',\n",
       " 'gemma3:12b',\n",
       " 'gemma3:27b',\n",
       " 'rnj-1:8b']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_models = client.list()\n",
    "[model.model for model in available_models.models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e90e6219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Model Response:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Since OpenRouter is compatible with the standard OpenAI API, the easiest way to connect is using the official **OpenAI SDK** (in Python or Node.js) but changing the `base_url`.\n",
       "\n",
       "Here are the solutions for **Python**, **Node.js**, and **LangChain**.\n",
       "\n",
       "### Prerequisites\n",
       "1.  Get an API Key from [OpenRouter Keys](https://openrouter.ai/keys).\n",
       "2.  Select a model ID from [OpenRouter Models](https://openrouter.ai/models) (e.g., `meta-llama/llama-3.1-8b-instruct`).\n",
       "\n",
       "---\n",
       "\n",
       "### Option 1: Python (Recommended)\n",
       "\n",
       "First, install the OpenAI library:\n",
       "```bash\n",
       "pip install openai\n",
       "```\n",
       "\n",
       "**The Code:**\n",
       "\n",
       "```python\n",
       "import os\n",
       "from openai import OpenAI\n",
       "\n",
       "# It is best practice to use environment variables, \n",
       "# but you can hardcode your key here for testing.\n",
       "OPENROUTER_API_KEY = \"sk-or-v1-...\" \n",
       "\n",
       "client = OpenAI(\n",
       "  base_url=\"https://openrouter.ai/api/v1\",\n",
       "  api_key=OPENROUTER_API_KEY,\n",
       "  # Optional: OpenRouter-specific headers for ranking and statistics\n",
       "  default_headers={\n",
       "      \"HTTP-Referer\": \"https://your-site-url.com\", # Optional: Your site URL\n",
       "      \"X-Title\": \"My App Name\", # Optional: Your app name\n",
       "  }\n",
       ")\n",
       "\n",
       "completion = client.chat.completions.create(\n",
       "  # OpenRouter requires the format 'vendor/model-name'\n",
       "  model=\"meta-llama/llama-3.1-8b-instruct:free\", \n",
       "  messages=[\n",
       "    {\n",
       "      \"role\": \"user\",\n",
       "      \"content\": \"Explain quantum computing in one sentence.\"\n",
       "    }\n",
       "  ]\n",
       ")\n",
       "\n",
       "print(completion.choices[0].message.content)\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "### Option 2: JavaScript / Node.js\n",
       "\n",
       "First, install the OpenAI npm package:\n",
       "```bash\n",
       "npm install openai\n",
       "```\n",
       "\n",
       "**The Code:**\n",
       "\n",
       "```javascript\n",
       "import OpenAI from \"openai\";\n",
       "\n",
       "const openai = new OpenAI({\n",
       "  baseURL: \"https://openrouter.ai/api/v1\",\n",
       "  apiKey: \"sk-or-v1-...\", // Your OpenRouter Key\n",
       "  defaultHeaders: {\n",
       "    \"HTTP-Referer\": \"https://your-site-url.com\", // Optional\n",
       "    \"X-Title\": \"My App Name\", // Optional\n",
       "  }\n",
       "});\n",
       "\n",
       "async function main() {\n",
       "  const completion = await openai.chat.completions.create({\n",
       "    model: \"meta-llama/llama-3.1-8b-instruct:free\",\n",
       "    messages: [\n",
       "      { role: \"user\", content: \"Write a haiku about code.\" }\n",
       "    ],\n",
       "  });\n",
       "\n",
       "  console.log(completion.choices[0].message.content);\n",
       "}\n",
       "\n",
       "main();\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "### Option 3: Python with LangChain\n",
       "\n",
       "If you are building an agent or a more complex app, you might use LangChain.\n",
       "\n",
       "```bash\n",
       "pip install langchain-openai\n",
       "```\n",
       "\n",
       "**The Code:**\n",
       "\n",
       "```python\n",
       "from langchain_openai import ChatOpenAI\n",
       "\n",
       "llm = ChatOpenAI(\n",
       "    base_url=\"https://openrouter.ai/api/v1\",\n",
       "    api_key=\"sk-or-v1-...\",\n",
       "    model=\"anthropic/claude-3.5-sonnet\", # Example model\n",
       ")\n",
       "\n",
       "response = llm.invoke(\"What is the capital of France?\")\n",
       "print(response.content)\n",
       "```\n",
       "\n",
       "### Key Differences from Standard OpenAI\n",
       "1.  **Base URL:** You must set it to `https://openrouter.ai/api/v1`.\n",
       "2.  **Model Names:** You cannot just say `gpt-4`. You must use the OpenRouter namespace format (e.g., `openai/gpt-4o`, `anthropic/claude-3-opus`, `google/gemini-pro-1.5`).\n",
       "3.  **Headers:** OpenRouter requests `HTTP-Referer` and `X-Title` to include your app in their leaderboards, though the code will work without them."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from ollama import Client\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "class OllamaCloudConnector:\n",
    "    def __init__(self, api_key=None):\n",
    "        self.api_key = api_key or os.getenv('OLLAMA_API_KEY')\n",
    "\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"<b>Ollama API Key is required</b>\")\n",
    "\n",
    "        self.client = Client(\n",
    "            host='https://ollama.com',\n",
    "            headers={'Authorization': f'Bearer {self.api_key}'}\n",
    "        )\n",
    "\n",
    "    def generate_text(self, model='gpt-oss:120b-cloud', prompt=''):\n",
    "        \"\"\"\n",
    "        Generate text using a cloud-based Ollama model\n",
    "\n",
    "        Args:\n",
    "            model (str): Cloud model to use\n",
    "            prompt (str): Input prompt for text generation\n",
    "\n",
    "        Returns:\n",
    "            str: Generated text response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.client.chat(\n",
    "                model=model,\n",
    "                messages=[{\n",
    "                    'role': 'user',\n",
    "                    'content': prompt\n",
    "                }],\n",
    "                stream=False \n",
    "            )\n",
    "            return response['message']['content']\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"<b>Error connecting to Ollama Cloud:</b> {e}\")\n",
    "            return None\n",
    "\n",
    "    def list_available_models(self):\n",
    "        \"\"\"\n",
    "        List available cloud models\n",
    "\n",
    "        Returns:\n",
    "            list: Available cloud model names\n",
    "        \"\"\"\n",
    "        available_models = self.client.list()\n",
    "        cloud_models = [model.model for model in available_models.models]\n",
    "\n",
    "        return cloud_models\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "\n",
    "        ollama_connector = OllamaCloudConnector()\n",
    "\n",
    "        # # List available cloud models\n",
    "        # print(\"<b>Available Cloud Models:</b>\")\n",
    "        # for model in ollama_connector.list_available_models():\n",
    "        #     print(f\"- {model}\")\n",
    "\n",
    "        # Generate text using a cloud model\n",
    "        prompt = \"Write a code to connect openrouter cloud models\"\n",
    "        response = ollama_connector.generate_text(\n",
    "            model= 'gemini-3-pro-preview', #'glm-4.7',\n",
    "            prompt=prompt\n",
    "        )\n",
    "\n",
    "        display(Markdown(\"## Model Response:</b>\"))\n",
    "        display(Markdown(response))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"<b>Error in Ollama Cloud Connection:</b> {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
